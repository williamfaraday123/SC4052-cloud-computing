{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a0ff460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: evaluate in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from transformers) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: psutil in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from accelerate>=0.20.3->transformers) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: six in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\data\\isaac\\ntu\\sc1015\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers transformers[torch] datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc193ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949018a",
   "metadata": {},
   "source": [
    "### Use Hugging Face to download a pre-trained code model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44394e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Data\\Isaac\\NTU\\SC1015\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Data\\Isaac\\NTU\\SC1015\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Data\\Isaac\\NTU\\SC1015\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Data\\Isaac\\NTU\\SC1015\\Lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/CodeGPT-small-py\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c08e9",
   "metadata": {},
   "source": [
    "### Import dataset  to train and evaluate the later finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4328a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'entry_point': 'has_close_elements', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'language': 'python', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'description': 'Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = \"./HumanEval.jsonl\"\n",
    "\n",
    "# Read and parse the JSONL file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Display the first example\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dacbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Dataset object\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba6fa1",
   "metadata": {},
   "source": [
    "### Preprocess the Dataset to tokenize and format it for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c0e86",
   "metadata": {},
   "source": [
    "Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3298065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70a3da",
   "metadata": {},
   "source": [
    "Split into Train and Evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2) # 20% for testing, 80% for training\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b08da",
   "metadata": {},
   "source": [
    "### Fine tune the model\n",
    "set up a training loop using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a83052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "class CodeDataset(TorchDataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]  # Get the row correctly\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"input_ids\"], dtype=torch.long)  # Labels should match input_ids for causal LM\n",
    "        }\n",
    "\n",
    "# Convert the Hugging Face dataset to PyTorch dataset\n",
    "train_dataset = CodeDataset(train_dataset)\n",
    "eval_dataset = CodeDataset(eval_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a7804",
   "metadata": {},
   "source": [
    "Define the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  # Shifted labels for causal LM\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35c52f",
   "metadata": {},
   "source": [
    "After training, save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eab473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_CodeGPT\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_CodeGPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610da7a1",
   "metadata": {},
   "source": [
    "### Benchmark and Evaluate the Model\n",
    "After fine-tuning, evaluate using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "print(f\"Evaluation Loss: {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37224b5e",
   "metadata": {},
   "source": [
    "### Test the fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_length=200)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_code(\"def fibonacci(n):\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
